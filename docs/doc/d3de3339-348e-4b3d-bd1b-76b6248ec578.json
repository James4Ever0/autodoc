{
    "summary": "This code defines a prompt template for AI assistants in software projects, providing guidelines for informative answers. It initializes a function, makeChain, to create chat chains using OpenAI API and LLMs models like GPT-4 or GPT-3, focusing on specific content types.",
    "details": [
        {
            "comment": "This code imports necessary modules and defines a prompt template for generating standalone questions based on chat history. The `makeQAPrompt` function creates a prompt template for an AI assistant in the context of a software project, training it with relevant content types located at a specified URL.",
            "location": "\"/media/root/Prima/works/autodoc/docs/src/src/cli/commands/query/createChatChain.ts\":0-19",
            "content": "import { OpenAIChat } from 'langchain/llms';\nimport { LLMChain, ChatVectorDBQAChain, loadQAChain } from 'langchain/chains';\nimport { PromptTemplate } from 'langchain/prompts';\nimport { HNSWLib } from '../../../langchain/hnswlib.js';\nimport { LLMModels } from '../../../types.js';\nconst CONDENSE_PROMPT =\n  PromptTemplate.fromTemplate(`Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\nChat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone question:`);\n// eslint-disable-next-line prettier/prettier\nconst makeQAPrompt = (projectName: string, repositoryUrl: string, contentType: string, chatPrompt: string, targetAudience: string) =>\n  PromptTemplate.fromTemplate(\n    `You are an AI assistant for a software project called ${projectName}. You are trained on all the ${contentType} that makes up this project.\n  The ${contentType} for the project is located at ${repositoryUrl}.\nYou are given the following extracted parts of a technical summary of files in a ${contentType} and a question. "
        },
        {
            "comment": "This code is providing guidelines for writing an informative and relevant answer, specifically addressing the target audience, including examples, avoiding context references, keeping response length appropriate, and maintaining focus on the project in question.",
            "location": "\"/media/root/Prima/works/autodoc/docs/src/src/cli/commands/query/createChatChain.ts\":20-30",
            "content": "Provide a conversational answer with hyperlinks back to GitHub.\nYou should only use hyperlinks that are explicitly listed in the context. Do NOT make up a hyperlink that is not listed.\nInclude lots of ${contentType} examples and links to the ${contentType} examples, where appropriate.\nAssume the reader is a ${targetAudience} but is not deeply familiar with ${projectName}.\nAssume the reader does not know anything about how the project is strucuted or which folders/files are provided in the context.\nDo not reference the context in your answer. Instead use the context to inform your answer.\nIf you don't know the answer, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.\nIf the question is not about the ${projectName}, politely inform them that you are tuned to only answer questions about the ${projectName}.\nYour answer should be at least 100 words and no more than 300 words.\nDo not include information that is not directly relevant to the question, even if the context includes it.\nAlways "
        },
        {
            "comment": "This code is initializing a function, makeChain, which takes various parameters to create a chat chain for answering questions about a specific content type. It utilizes the OpenAI API and generates prompts for the chat chain. The LLMs models are GPT-4 or GPT-3.",
            "location": "\"/media/root/Prima/works/autodoc/docs/src/src/cli/commands/query/createChatChain.ts\":30-72",
            "content": "include a list of reference links to GitHub from the context. Links should ONLY come from the context.\n${\n  chatPrompt.length > 0\n    ? `Here are some additional instructions for answering questions about ${contentType}:\\n${chatPrompt}`\n    : ''\n}\nQuestion: {question}\nContext:\n{context}\nAnswer in Markdown:`,\n  );\nexport const makeChain = (\n  projectName: string,\n  repositoryUrl: string,\n  contentType: string,\n  chatPrompt: string,\n  targetAudience: string,\n  vectorstore: HNSWLib,\n  llms: LLMModels[],\n  onTokenStream?: (token: string) => void,\n) => {\n  /**\n   * GPT-4 or GPT-3\n   */\n  const llm = llms?.[1] ?? llms[0];\n  const questionGenerator = new LLMChain({\n    llm: new OpenAIChat({ temperature: 0.1, modelName: llm }),\n    prompt: CONDENSE_PROMPT,\n  });\n  // eslint-disable-next-line prettier/prettier\n  const QA_PROMPT = makeQAPrompt(projectName, repositoryUrl, contentType, chatPrompt, targetAudience);\n  const docChain = loadQAChain(\n    new OpenAIChat({\n      temperature: 0.2,\n      frequencyPenalty: 0,\n      presencePenalty: 0,"
        },
        {
            "comment": "The code creates a new ChatVectorDBQAChain instance using the specified model, streaming flag, and callback manager. It also includes a prompt and returns the newly created chain for further processing or execution.",
            "location": "\"/media/root/Prima/works/autodoc/docs/src/src/cli/commands/query/createChatChain.ts\":73-89",
            "content": "      modelName: llm,\n      streaming: Boolean(onTokenStream),\n      callbackManager: {\n        handleLLMNewToken: onTokenStream,\n        handleLLMStart: () => null,\n        handleLLMEnd: () => null,\n      } as any,\n    }),\n    { prompt: QA_PROMPT },\n  );\n  return new ChatVectorDBQAChain({\n    vectorstore,\n    combineDocumentsChain: docChain,\n    questionGeneratorChain: questionGenerator,\n  });\n};"
        }
    ]
}