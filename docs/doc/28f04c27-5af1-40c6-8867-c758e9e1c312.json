{
    "summary": "Autodoc is an open-source tool using LLMs to generate codebase documentation. It can be installed in repositories and utilizes depth-first traversal for specific answers, with Node version requirements outlined and command usage detailed. Early-stage project with pricing and future model support information provided, along with guidance on querying and community engagement, while contribution details are found in the CONTRIBUTING.md file.",
    "details": [
        {
            "comment": "This code is for the README of a project called Autodoc, which generates documentation for codebases using LLMs (Large Language Models). It has an MIT license, can be found on NPM with the package \"@context-labs/autodoc\", and has a Twitter and Discord presence.",
            "location": "\"/media/root/Prima/works/autodoc/docs/src/README.md\":1-21",
            "content": "<h1 align=\"center\">\n  <br>\n  <a href=\"https://github.com/context-labs/autodoc\"><img src=\"https://raw.githubusercontent.com/context-labs/autodoc/master/assets/autodoc.png\" alt=\"Markdownify\" width=\"200\" style=\"border-radius:8px;\"></a>\n  <br>\nAutodoc\n  <br>\n</h1>\n<h4 align=\"center\">\u26a1 Toolkit for auto-generating codebase documentation using LLMs \u26a1</h4>\n<p align=\"center\">\n<a href=\"https://opensource.org/licenses/MIT\">\n\t  <img alt=\"Twitter URL\" src=\"https://img.shields.io/badge/License-MIT-yellow.svg\">\n  </a>\n\t<a href=\"https://www.npmjs.com/package/@context-labs/autodoc\">\n\t  <img alt=\"NPM Package\" src=\"https://badge.fury.io/js/@context-labs%2Fautodoc.svg\">\n  </a>\n  <a href=\"https://twitter.com/autodoc_\">\n\t  <img alt=\"Twitter URL\" src=\"https://img.shields.io/twitter/url?label=Follow%20%40autodoc_&style=social&url=https%3A%2F%2Ftwitter.com%2Fautodoc_\">\n\t  <a href=\"https://discord.com/invite/zpFEXXWSNg\">\n\t  <img alt=\"Discord Server\" src=\"https://dcbadge.vercel.app/api/server/zpFEXXWSNg?compact=true&style=flat\">\n  </a>"
        },
        {
            "comment": "This code snippet is an introduction to Autodoc, a toolkit for auto-generating documentation using Large Language Models. It can be installed in a repository and indexes the codebase through depth-first traversal. The generated documentation stays with the codebase, allowing developers to ask questions about the code and receive specific answers with code file references.",
            "location": "\"/media/root/Prima/works/autodoc/docs/src/README.md\":22-35",
            "content": "</p>\n<p align=\"center\">\n  <a href=\"#what-is-this\">What is this?</a> \u2022\n  <a href=\"#get-started\">Get Started</a> \u2022\n  <a href=\"#community\">Community</a> \u2022\n  <a href=\"#contributing\">Contribute</a>\n</p>\n## What is this?\nAutodoc is a **experimental** toolkit for auto-generating codebase documentation for git repositories using Large Language Models, like [GPT-4](https://openai.com/research/gpt-4) or [Alpaca](https://github.com/ggerganov/llama.cpp). Autodoc can be [installed](#get-started) in your repo in about 5 minutes. It indexes your codebase through a depth-first traversal of all repository contents and calls an LLM to write documentation for each file and folder. These documents can be combined to describe the different components of your system and how they work together. \nThe generated documentation lives in your codebase, and travels where your code travels. Developers who download your code can use the `doc` command to ask questions about your codebase and get highly specific answers with reference links back to code files. "
        },
        {
            "comment": "The code provides information about the status of Autodoc, which is in the early stages of development and not ready for production use. It also mentions an issue to contribute to if interested and explains that response quality may vary due to project type and question phrasing.",
            "location": "\"/media/root/Prima/works/autodoc/docs/src/README.md\":37-47",
            "content": "In the near future, documentation will be re-indexed as part your CI pipeline, so it is always up-to-date. If your interested in working contributing to this work, see [this issue](https://github.com/context-labs/autodoc/issues/7).\n### Status\nAutodoc is in the early stages of development. It is functional, but not ready for production use. Things may break, or not work as expected. If you're interested in working on the core Autodoc framework, please see [contributing](#contributing). We would love to have your help!\n### FAQs\n**Question:** I'm not getting good responses. How can I improve response quality?\n**Answer:** Autodoc is in the early stages of development. As such, the response quality can vary widely based on the type of project your indexing and how questions are phrased. A few tips to writing good query:\n1. Be specific with your questions. Ask things like \"What are the different components of authorization in this system?\" rather than \"explain auth\". This will help Autodoc select the right context to get the best answer for your question."
        },
        {
            "comment": "Explanation: The code is providing an example of using the Autodoc tool to generate documentation. It mentions two examples, including a repository and a chatbot, both generated by Autodoc. Additionally, it specifies the requirements for getting started with Autodoc, which includes Node version 18.0.0 or greater, with version 19.0.0 or greater recommended.",
            "location": "\"/media/root/Prima/works/autodoc/docs/src/README.md\":48-65",
            "content": "2. Use GPT-4. GPT-4 is substantially better at understanding code compared to GPT-3.5 and this understanding carries over into writing good documentation as well. If you don't have access, sign up [here](https://openai.com/waitlist/gpt-4-api).\n### Examples\nBelow are a few examples of how Autodoc can be used. \n1. [Autodoc](https://github.com/context-labs/autodoc) - This repository contains documentation for itself, generated by Autodoc. It lives in the `.autodoc` folder. Follow the instructions [here](#querying) to learn how to query it.\n2. [TolyGPT.com](https://tolygpt.com) - TolyGPT is an Autodoc chatbot trained on the [Solana validator](https://github.com/solana-labs/solana) codebase and deployed to the web for easy access. In the near future, Autodoc will support a web version in addition to the existing CLI tool.\n## Get Started\n#### Requirements\nAutodoc requires Node v18.0.0 or greater. v19.0.0 or greater is recommended. Make sure you're running the proper version:\n```bash\n$ node -v\n```\nExample output:"
        },
        {
            "comment": "Install Autodoc CLI as global NPM module with `npm install -g @context-labs/autodoc`. Use the CLI to query a repository that has Autodoc installed. Ensure OpenAI API key is exported. Start the Autodoc query CLI with `doc q`, then select appropriate GPT model for access level if prompted.",
            "location": "\"/media/root/Prima/works/autodoc/docs/src/README.md\":66-101",
            "content": "```bash\nv19.8.1\n```\nInstall the Autodoc CLI tool as a global NPM module:\n```bash\n$ npm install -g @context-labs/autodoc\n```\nThis command installs the Autodoc CLI tool that will allow you to create and query Autodoc indexes.\nRun `doc` to see the available commands.\n### Querying\nYou can query a repository that has Autodoc installed via the CLI. We'll use the Autodoc repository itself as an example to demonstrate how querying in Autodoc works, but this could be your own repository that contains an index.\nClone Autodoc and change directory to get started:\n```bash \n$ git clone https://github.com/context-labs/autodoc.git\n$ cd autodoc\n```\nRight now Autodoc only supports OpenAI. Make sure you have have your OpenAI API key exported in your current session:\n```bash\n$ export OPENAI_API_KEY=<YOUR_KEY_HERE>\n```\nTo start the Autodoc query CLI, run:\n```bash\n$ doc q\n```\nIf this is your first time running `doc q`, you'll get a screen that prompts you to select which GPT models you have access to. Select whichever is appropriate for your level of access. If you aren't sure, select the first option:"
        },
        {
            "comment": "Code snippet describes the steps to generate documentation for a project using Autodoc. It outlines changing directory into the root of the project, setting up OpenAI API key, and running the `init` command. User will be prompted to enter their GitHub username and password.",
            "location": "\"/media/root/Prima/works/autodoc/docs/src/README.md\":103-128",
            "content": "<img src=\"https://raw.githubusercontent.com/context-labs/autodoc/master/assets/select-models.png\" alt=\"Markdownify\" width=\"60%\" style=\"border-radius:24px;\">\nYou're now ready to query documentation for the Autodoc repository:\n<img src=\"https://raw.githubusercontent.com/context-labs/autodoc/master/assets/query.gif\" alt=\"Markdownify\" width=\"60%\" style=\"border-radius:24px;\">\nThis is the core querying experience. It's very basic right now, with plenty of room of improvement. If you're interested in improving the Autodoc CLI querying experience, checkout [this issue](https://github.com/context-labs/autodoc/issues/11).\n### Indexing\nFollow the steps below to generate documentation for your own repository using Autodoc.\nChange directory into the root of your project:\n```bash\ncd $PROJECT_ROOT\n```\nMake sure your OpenAI API key is available in the current session:\n```bash\n$ export OPENAI_API_KEY=<YOUR_KEY_HERE>\n```\nRun the `init` command:\n```\ndoc init\n```\nYou will be prompted to enter the name of your project, GitH"
        },
        {
            "comment": "This code prompts the user to enter URLs and select GPT models for analysis. It then generates an `autodoc.config.json` file in the project's root directory to store these values, which should be committed to Git. The current prompts are developer-focused and assume a code-centered repository. Users can find prompt directions in `prompts.ts`, with customizable snippets in `autodoc.config.json`.",
            "location": "\"/media/root/Prima/works/autodoc/docs/src/README.md\":128-143",
            "content": "ub url, and select which GPT models you have access to. If you aren't sure which models you have access to, select the first option. You can also specify your own GPT file/directory prompts that will be used to summarize/analyze the code repoThis command will generate an `autodoc.config.json` file in the root of your project to store the values. This file should be checked in to git.\n**Note:** Do not skip entering these values or indexing may not work.\n**Prompt Configuration:** You'll find prompt directions specified in `prompts.ts`, with some snippets customizable in the `autodoc.config.json`. The current prompts are developer focused and assume your repo is code focused. We will have more reference templates in the future.\nRun the `index` command:\n```bash\ndoc index\n```\nYou should see a screen like this:\n<img src=\"https://raw.githubusercontent.com/context-labs/autodoc/master/assets/index-estimate.png\" alt=\"Markdownify\" width=\"60%\" style=\"border-radius:24px;\">\nThis screen estimates the cost of "
        },
        {
            "comment": "This code snippet explains the indexing process in Autodoc and its model selection strategy. The `doc index` command is used for indexing a repository, reindexing only changed files on subsequent runs. The number of tokens in each file determines which model to use, with GPT-3.5 being less accurate for files under 4000 tokens. It is recommended to use GPT-4 8K for better output, and users can apply for access at the provided link.",
            "location": "\"/media/root/Prima/works/autodoc/docs/src/README.md\":143-147",
            "content": "indexing your repository. You can also access this screen via the `doc estimate` command. If you've already indexed once, then `doc index` will only reindex files that have been changed on the second go.\nFor every file in your project, Autodoc calculates the number of tokens in the file based on the file content. The more lines of code, the larger the number of tokens. Using this number, it determine which model it will use on per file basis, always choosing the cheapest model whose context length supports the number of tokens in the file. If you're interested in helping make model selection configurable in Autodoc, check out [this issue](https://github.com/context-labs/autodoc/issues/9).\n**Note:** This naive model selection strategy means that files under ~4000 tokens will be documented using GPT-3.5, which will result in less accurate documentation. **We recommend using GPT-4 8K at a minimum.** Indexing with GPT-4 results in significantly better output. You can apply for access [here](https://openai.com/waitlist/gpt-4-api)."
        },
        {
            "comment": "Cost and OpenAI pricing: Code mentions that for large projects, the cost can be several hundred dollars. It also provides a link to view OpenAI pricing.\n\nFuture support for self-hosted models: Code states future support for models like Llama and Alpaca, and refers to an issue where interested contributors can participate.\n\nRepository indexing completion: The code describes the expected appearance of the screen once the repository is done being indexed.\n\nQuerying process: Code instructs users to follow a certain procedure for querying their application after indexing completion.\n\nCommunity engagement: Code invites users to join the Discord group and follow Twitter for updates, and encourages contributions from interested individuals by referring to an issue.",
            "location": "\"/media/root/Prima/works/autodoc/docs/src/README.md\":149-160",
            "content": "For large projects, the cost can be several hundred dollars. View OpenAI pricing [here](https://openai.com/pricing). \nIn the near future, we will support self-hosted models, such as [Llama](https://github.com/facebookresearch/llama) and [Alpaca](https://github.com/tatsu-lab/stanford_alpaca). Read [this issue](https://github.com/context-labs/autodoc/issues/8) if you're interesting in contributing to this work.\nWhen your repository is done being indexed, you should see a screen like this:\n<img src=\"https://raw.githubusercontent.com/context-labs/autodoc/master/assets/index-finished.png\" alt=\"Markdownify\" width=\"60%\" style=\"border-radius:24px;\">\nYou can now query your application using the steps outlined in [querying](#querying).\n## Community\nThere is a small group of us that are working full time on Autodoc. Join us on [Discord](https://discord.gg/zpFEXXWSNg), or follow us on [Twitter](https://twitter.com/autodoc_) for updates. We'll be posting regularly and continuing to improve the Autodoc application. Want to contribute? Read below."
        },
        {
            "comment": "This code snippet is a brief introduction to contributing to an open source project. It encourages contributions in the form of new features, improved infrastructure, or better documentation. The detailed information on how to contribute can be found in the [CONTRIBUTING.md](.github/CONTRIBUTING.md) file.",
            "location": "\"/media/root/Prima/works/autodoc/docs/src/README.md\":163-167",
            "content": "## Contributing\nAs an open source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infra, or better documentation.\nFor detailed information on how to contribute, see [here](.github/CONTRIBUTING.md)."
        }
    ]
}