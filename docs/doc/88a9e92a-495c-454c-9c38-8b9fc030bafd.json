{
    "summary": "The code sets up counters, initializes an LLMUtil object, and calculates total index cost estimates for LLM models, generating a summary table of results.",
    "details": [
        {
            "comment": "This code defines a constant \"models\" which is a record containing information about various LLMs (Language Learning Models) such as GPT3, GPT4, and GPT432k. Each model has properties like name, input/output costs per 1K tokens, maximum length, an instance of the OpenAIChat LLM class with temperature and modelName set, and counters for successes, failures, and total requests.",
            "location": "\"/media/root/Prima/works/autodoc/docs/src/src/cli/utils/LLMUtil.ts\":0-40",
            "content": "import { OpenAIChat } from 'langchain/llms';\nimport { LLMModelDetails, LLMModels } from '../../types.js';\nexport const models: Record<LLMModels, LLMModelDetails> = {\n  [LLMModels.GPT3]: {\n    name: LLMModels.GPT3,\n    inputCostPer1KTokens: 0.0015,\n    outputCostPer1KTokens: 0.002,\n    maxLength: 3050,\n    llm: new OpenAIChat({\n      temperature: 0.1,\n      openAIApiKey: process.env.OPENAI_API_KEY,\n      modelName: LLMModels.GPT3,\n    }),\n    inputTokens: 0,\n    outputTokens: 0,\n    succeeded: 0,\n    failed: 0,\n    total: 0,\n  },\n  [LLMModels.GPT4]: {\n    name: LLMModels.GPT4,\n    inputCostPer1KTokens: 0.03,\n    outputCostPer1KTokens: 0.06,\n    maxLength: 8192,\n    llm: new OpenAIChat({\n      temperature: 0.1,\n      openAIApiKey: process.env.OPENAI_API_KEY,\n      modelName: LLMModels.GPT4,\n    }),\n    inputTokens: 0,\n    outputTokens: 0,\n    succeeded: 0,\n    failed: 0,\n    total: 0,\n  },\n  [LLMModels.GPT432k]: {\n    name: LLMModels.GPT432k,\n    inputCostPer1KTokens: 0.06,\n    outputCostPer1KTokens: 0.12,\n    maxLength: 32768,"
        },
        {
            "comment": "The code initializes an LLMUtil object with OpenAIChat instance and counters for input/output tokens, succeeded/failed requests, and total requests. It then defines a function printModelDetails that maps through the provided models array to generate output objects containing details like model name, file count, success/failure counts, and cost based on token usage. The function also calculates totals by reducing the output arrays.",
            "location": "\"/media/root/Prima/works/autodoc/docs/src/src/cli/utils/LLMUtil.ts\":41-80",
            "content": "    llm: new OpenAIChat({\n      temperature: 0.1,\n      openAIApiKey: process.env.OPENAI_API_KEY,\n      modelName: LLMModels.GPT4,\n    }),\n    inputTokens: 0,\n    outputTokens: 0,\n    succeeded: 0,\n    failed: 0,\n    total: 0,\n  },\n};\nexport const printModelDetails = (models: LLMModelDetails[]): void => {\n  const output = models.map((model) => {\n    return {\n      Model: model.name,\n      'File Count': model.total,\n      Succeeded: model.succeeded,\n      Failed: model.failed,\n      Tokens: model.inputTokens + model.outputTokens,\n      Cost:\n        (model.inputTokens / 1000) * model.inputCostPer1KTokens +\n        (model.outputTokens / 1000) * model.outputCostPer1KTokens,\n    };\n  });\n  const totals = output.reduce(\n    (cur: any, next) => {\n      return {\n        ...cur,\n        'File Count': cur['File Count'] + next['File Count'],\n        Succeeded: cur.Succeeded + next.Succeeded,\n        Failed: cur.Failed + next.Failed,\n        Tokens: cur.Tokens + next.Tokens,\n        Cost: cur.Cost + next.Cost,\n      };\n    },\n    {\n      Model: 'Total',"
        },
        {
            "comment": "The code calculates the total index cost estimate for LLM models, creating a summary table of results including file count, succeeded, failed counts, and token costs.",
            "location": "\"/media/root/Prima/works/autodoc/docs/src/src/cli/utils/LLMUtil.ts\":81-103",
            "content": "      'File Count': 0,\n      Succeeded: 0,\n      Failed: 0,\n      Tokens: 0,\n      Cost: 0,\n    },\n  );\n  const all = [...output, totals];\n  console.table(all);\n};\nexport const totalIndexCostEstimate = (models: LLMModelDetails[]): number => {\n  const totalCost = models.reduce((cur, model) => {\n    return (\n      cur +\n      (model.inputTokens / 1000) * model.inputCostPer1KTokens +\n      (model.outputTokens / 1000) * model.outputCostPer1KTokens\n    );\n  }, 0);\n  return totalCost;\n};"
        }
    ]
}